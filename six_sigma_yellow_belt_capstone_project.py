# -*- coding: utf-8 -*-
"""SIX SIGMA YELLOW BELT CAPSTONE PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t-JIjc9tpm6HmfDimMTckbJrdgg6Ixhd

# SIX SIGMA YELLOW BELT CAPSTONE PROJECT
## Power BI Dashboard Development Process Optimization

---

# 1. PROJECT CHARTER

## Project Information
- **Project Name:** Power BI Dashboard Development Process Optimization
- **Project Sponsor:** Sarah Mitchell, VP of Business Intelligence
- **Team Lead:** Sai Krishna Sajjanam
- **Today's Date:** October 28, 2025
- **Project Start Date:** November 1, 2025
- **Target Completion Date:** February 28, 2026
- **Project Duration:** 4 months (Define & Measure phases - Yellow Belt Scope)

---

## Problem Statement
Over the past 6 months, the average dashboard development cycle time has been **45 days** from requirements gathering to deployment, which is **30% longer** than the industry benchmark of 32 days. During this period, the requirements rework rate is **15%**, resulting in an average of 7 additional days for corrections. This delay impacts stakeholder decision-making capabilities and results in an estimated annual cost of **â‚¬50,000 in lost productivity** and delayed insights. Additionally, stakeholder satisfaction (NPS) has plateaued at 6.2/10, indicating dissatisfaction with delivery timelines and process consistency.

---

## Business Case

**Why is this project important to do now?**
Business stakeholders are increasingly demanding real-time insights for strategic decision-making in competitive markets. The Financial Services Division has explicitly stated that delayed dashboard delivery prevents timely responses to market changes and reduces our competitive advantage. Three major initiatives scheduled for Q4 2025 require accelerated BI support that our current 45-day cycle cannot accommodate.

**What is the project's financial impact?**
Reducing dashboard development cycle time by 29% (from 45 to 32 days) will generate approximately â‚¬40,000 in annual savings through:
- Improved team productivity (reduced rework-related labor hours)
- Faster time-to-insights enabling better business decisions (estimated 5-10% improvement in decision quality)
- Reduced opportunity costs from delayed analytics

Additionally, reducing rework rate from 15% to 6% will eliminate approximately â‚¬8,000 annually in redundant labor and revision cycles.

**What is the impact on DPMO/Sigma level?**
Current defect rate (rework requiring significant revision) = 15% = 150,000 DPMO â‰ˆ 2.8 Sigma
Target defect rate = 6% = 60,000 DPMO â‰ˆ 3.5 Sigma
This represents moving from "Poor" to "Acceptable" performance on the Six Sigma scale.

**What is the impact on customer service?**
Current stakeholder satisfaction (NPS) = 6.2/10 (Detractors: 35%, Passives: 45%, Promoters: 20%)
Target stakeholder satisfaction (NPS) = 8.5/10 (Detractors: 10%, Passives: 20%, Promoters: 70%)
Faster delivery with fewer rework cycles will improve stakeholder experience and increase likelihood of referrals and expanded BI adoption across departments.

---

## Goal Statement (SMART)

**Reduce Power BI dashboard development cycle time from 45 days to 32 days (29% reduction) and decrease requirements rework rate from 15% to 6% by February 28, 2026, through process standardization, requirements template implementation, and Agile sprint methodology adoption.**

### SMART Analysis:
- **Specific:** Dashboard development process for internal business stakeholders using Power BI
- **Measurable:** Cycle time (days), rework rate (%), on-time delivery rate (%), satisfaction (NPS)
- **Achievable:** Based on industry benchmarks and proven best practices
- **Relevant:** Supports organizational strategic goals and customer satisfaction
- **Time-bound:** 4-month project timeline with February 28, 2026 target

---

## Improvement Goals

| # | Measure | Current Baseline | Target Goal | Target Date |
|---|---------|-----------------|-------------|-------------|
| 1 | Dashboard development cycle time (days) | 45 days | 32 days | Feb 28, 2026 |
| 2 | Requirements rework rate (%) | 15% | 6% | Feb 28, 2026 |
| 3 | Stakeholder satisfaction (NPS score) | 6.2/10 | 8.5/10 | Feb 28, 2026 |
| 4 | On-time delivery rate (%) | 65% | 90% | Feb 28, 2026 |
| 5 | Requirements approval time (days) | 12 days | 5 days | Feb 28, 2026 |

---

## Process Description

The BI dashboard development process begins when a business stakeholder submits a dashboard request via the internal portal. The process follows these phases:

1. **Request Submission & Initial Assessment:** Stakeholder submits request. Team Lead reviews feasibility and resource availability.

2. **Kickoff Meeting:** Initial meeting scheduled with stakeholder to discuss objectives (Average wait time: 5 days; Meeting duration: 2 hours).

3. **Requirements Gathering:** Conducted through multiple stakeholder meetings (typically 2-3 sessions). Team discusses data needs, metrics, visualizations, and refresh frequency (Current duration: 12 days; Industry benchmark: 5 days).

4. **Requirements Documentation:** Business Analyst creates detailed requirements document including data sources, calculations, and design mockups.

5. **Stakeholder Review & Approval:** Stakeholder reviews documentation and provides approval or feedback (Current duration: 5 days).

6. **Data Source Identification:** Data Engineer identifies required data sources and evaluates ETL feasibility (Duration: 3 days).

7. **ETL Development:** Data pipelines are built in Informatica to extract, transform, and load data into the analytics data warehouse (Duration: 8 days).

8. **Dashboard Design & Development:** Power BI Developer designs dashboard layout and develops interactive visualizations using DAX calculations (Duration: 10 days).

9. **Internal QA Testing:** QA Analyst tests dashboard functionality, data accuracy, and performance (Duration: 3 days).

10. **User Acceptance Testing (UAT):** Stakeholder tests the dashboard in the staging environment and provides feedback (Current duration: 5 days; Current rework rate: 15%).

11. **Rework (if required):** Issues identified during UAT are addressed by the development team (Average rework duration when required: 7 days; Rework frequency: 15% of projects).

12. **Deployment to Production:** Dashboard is deployed to the production Power BI environment and access permissions are configured (Duration: 1 day).

13. **Training & Handover:** End users receive training on dashboard navigation, filters, and data interpretation (Duration: 1 day).

14. **Project Closure:** Project documentation is updated, lessons learned are captured, and the project is formally closed (Duration: 1 day).

**Total Process Cycle Time:** 45 days average (Range: 35-60 days depending on rework requirements and stakeholder availability)

---

## Project Scope

### What part of the process will be addressed?
This Yellow Belt project will focus on the Define and Measure phases of the DMAIC model, specifically addressing:
- Requirements gathering and documentation (Phase 1-2)
- Stakeholder review and approval processes (Phase 2-3)
- Dashboard design and development workflow (Phase 3-4)
- User acceptance testing and feedback incorporation (Phase 4-5)

The Improve, Control phases would be addressed in subsequent phases (Green Belt or Black Belt project).

### What are the specific boundaries of the project?

**IN SCOPE:**
- Power BI dashboard development process (excluding Tableau, QlikView, or other BI tools)
- Internal business stakeholder dashboards only
- Requirements gathering through deployment phases
- Process documentation and workflow analysis
- Data collection on cycle time, defect rates, and satisfaction
- Root cause analysis of delays and rework

**OUT OF SCOPE:**
- Infrastructure improvements (server capacity, licensing, network upgrades)
- Data warehouse architecture changes
- ETL pipeline optimization (separate project under Data Engineering)
- Budget allocation or hiring decisions
- Data governance policy changes
- Dashboards for external clients or customers

### What areas are inside or outside the team's focus or authority?

**INSIDE TEAM'S AUTHORITY:**
- Process documentation and standardization
- Developing requirements templates and checklists
- Workflow sequence and assignments
- Testing protocols and quality standards
- Training and documentation procedures
- Team communication and meeting structure

**OUTSIDE TEAM'S AUTHORITY:**
- IT infrastructure investments
- Software tool selection or licensing
- Personnel hiring/termination
- Cross-department budget reallocation
- Changes to data warehouse schema
- Organizational structure changes

**REQUIRES SPONSOR APPROVAL:**
- Changes to stakeholder involvement requirements
- Reallocating team members to other priorities
- Requesting additional tools or software
- Exceptions to established process standards

---

## Key Stakeholders

| Stakeholder | Role | Interest/Influence |
|-------------|------|-------------------|
| **Sarah Mitchell** | Project Sponsor, VP of BI | High - Funds and prioritizes projects; wants faster delivery |
| **Finance Director** | Stakeholder/Customer | High - Major consumer of dashboards; impacts ROI decisions |
| **Sales Operations Manager** | Stakeholder/Customer | High - Uses dashboards for pipeline management |
| **IT Director** | Stakeholder/Partner | Medium - Manages infrastructure and access controls |
| **David Chen** | Process Owner, BI Manager | High - Oversees dashboard team daily operations |
| **Data Governance Lead** | Stakeholder/Advisor | Medium - Ensures data quality and compliance standards |
| **Lean Six Sigma Black Belt** | Project Advisor | High - Provides guidance on DMAIC methodology |

---

## Project Team

| Role | Team Member | Experience | Responsibility |
|------|-------------|------------|-----------------|
| **Team Lead** | Sai Krishna Sajjanam | 5+ years BI analytics, Project Management | Overall project coordination; stakeholder communication; DMAIC execution |
| **Power BI Developer** | Emma O'Brien | 4 years Power BI development | Dashboard design review; development process analysis; testing protocols |
| **Business Analyst** | Raj Patel | 6 years BA experience | Requirements gathering analysis; process documentation; data collection |
| **Data Engineer** | Maria Santos | 5 years data pipeline development | ETL process analysis; data source documentation; technical feasibility assessment |
| **QA Analyst** | James Murphy | 3 years testing & quality assurance | UAT process analysis; defect tracking; quality metrics development |

---

## Project Timeline

| Phase | Key Milestones | Target Dates | Duration |
|-------|----------------|--------------|----------|
| **Define** | Project Charter completion; Stakeholder alignment; Team kickoff | Nov 1-15, 2025 | 2 weeks |
| **Measure** | Data collection plan finalized; Process mapping complete; Baseline data collected | Nov 15 - Dec 20, 2025 | 5 weeks |
| **Analyze** | Root cause analysis completed; Hypotheses validated; Recommendations prepared | Dec 20 - Jan 24, 2026 | 5 weeks |
| **Report & Present** | Final report and recommendations to Sponsor | Feb 28, 2026 | - |
| **Future: Improve** | (Green Belt or Black Belt scope) Implement improvements | Future project | - |
| **Future: Control** | (Green Belt or Black Belt scope) Standardize improvements | Future project | - |

### Project Schedule (Detailed)
- **Week 1 (Nov 1-8):** Kickoff meeting; Team formation; Charter finalization
- **Week 2 (Nov 8-15):** Stakeholder interviews; Initial process documentation
- **Week 3-4 (Nov 15-29):** Process mapping; Data collection plan development
- **Week 5-8 (Nov 29 - Dec 27):** Baseline data collection; Statistical analysis
- **Week 9-11 (Jan 1-24):** Root cause analysis; Hypothesis testing; Final report
- **Week 12 (Feb 28):** Final presentation to Sponsor

---

# 2. TEAM CHARTER

## Project Information
- **Project Title:** Power BI Dashboard Development Process Optimization
- **Team Lead:** Sai Krishna Sajjanam
- **Project Sponsor:** Sarah Mitchell
- **Date Prepared:** October 28, 2025
- **Team Composition:** 5 members + Black Belt Advisor

---

## Team Expectations & Ground Rules

### Meeting Attendance & Timeliness
**Expectation:** All team members are expected to attend weekly project meetings.

**Team Rules:**
- Mandatory attendance at all scheduled meetings (Tuesdays, 10:00 AM GMT)
- 24-hour advance notice required if unable to attend; no substitutes without Team Lead approval
- Meetings begin promptly at 10:00 AM; latecomers minimize disruption
- No virtual attendees joining 15+ minutes after start time due to confidentiality of discussion
- Maximum 2 absences permitted during 4-month project without review

---

### Meeting Focus & Preparation
**Expectation:** Meetings stay focused on project objectives. Team members come prepared.

**Team Rules:**
- Published agenda distributed 48 hours before each meeting via Teams channel
- All assigned tasks/deliverables completed 24 hours before meetings
- Data collection forms and weekly reports submitted on time (Friday EOD for Tuesday meetings)
- Off-topic discussions noted and addressed in separate meeting or via email
- Meeting notes recorded and shared within 24 hours of meeting conclusion

---

### Participation & Engagement
**Expectation:** Active participation from all team members; diverse perspectives encouraged.

**Team Rules:**
- Each team member contributes minimally once per meeting
- No side conversations or distractions during discussions
- Questions and concerns raised directly to Team Lead or in full group
- All team members have equal voice regardless of hierarchy
- Disagreements handled respectfully using data-driven reasoning

---

### Communication & Interruptions
**Expectation:** Professional and focused meeting environment.

**Team Rules:**
- Mobile phones on silent; no texting or emails during meetings
- Emergency calls only; if critical situation arises, step out briefly
- Teams channel muted except for critical project announcements
- Slack for casual updates; Email for formal documentation
- Weekly status reports submitted every Friday 4:00 PM GMT

---

### Data & Decision Making
**Expectation:** All recommendations and decisions backed by data.

**Team Rules:**
- Recommendations must be supported by quantitative data or evidence
- Anecdotal feedback noted separately from data-driven findings
- All claims include source, collection method, and sample size
- Decisions made using consensus-building approach when possible
- If consensus cannot be reached, Team Lead makes final decision after full discussion

---

### Conflict Resolution
**Expectation:** Respectful disagreement encouraged; conflicts managed professionally.

**Team Rules:**
- Disagreements focused on ideas, not individuals
- Direct communication preferred; escalate to Team Lead if unresolved after 2 discussions
- Black Belt Advisor available for conflict mediation if needed
- All conflicts documented in project record
- Resolution documented and communicated to all team members

---

### Confidentiality & Professionalism
**Expectation:** Project information treated as confidential and sensitive.

**Team Rules:**
- Project data and findings not shared outside team without Sponsor approval
- Stakeholder feedback discussed in context of project objectives
- Professional language in all communications (written and verbal)
- No side discussions with stakeholders without Team Lead knowledge
- All documents stored in secure project SharePoint folder

---

### Scope & Deliverables
**Expectation:** Clear understanding of project scope and deliverable ownership.

**Team Rules:**
- RACI matrix reviewed at kickoff (Responsible, Accountable, Consulted, Informed)
- Each team member owns specific deliverables with clear deadlines
- Status updates on deliverables required at each meeting
- If deliverable at risk, notify Team Lead immediately (not day before deadline)
- Quality standards discussed and agreed before development begins

---

## Team Member Commitment & Signatures

| Team Member | Role | Hours/Week | Signature | Date |
|-------------|------|-----------|-----------|------|
| Sai Krishna Sajjanam | Team Lead | 15 hours | _________________________ | 10/28/2025 |
| Emma O'Brien | Power BI Developer | 8 hours | _________________________ | 10/28/2025 |
| Raj Patel | Business Analyst | 10 hours | _________________________ | 10/28/2025 |
| Maria Santos | Data Engineer | 8 hours | _________________________ | 10/28/2025 |
| James Murphy | QA Analyst | 8 hours | _________________________ | 10/28/2025 |
| **Sarah Mitchell** | **Sponsor** | 2 hours | _________________________ | 10/28/2025 |

---

## Team Norms
- **Decision Making:** Data-driven, consensus-based when possible
- **Communication Style:** Clear, direct, respectful
- **Problem Solving:** Root cause analysis before proposing solutions
- **Documentation:** All decisions and findings recorded
- **Learning:** Share knowledge; help colleagues succeed
- **Accountability:** Own commitments; communicate early if issues arise

---

# 3. DATA COLLECTION PLAN

## Overview
This data collection plan specifies what data will be collected, from what sources, using what methods, and with what frequency to establish baseline metrics for the Power BI Dashboard Development Process Optimization project.

---

## Data Collection Matrix

| # | What to Measure | Metric | Data Type | Collection Method | Data Source | Frequency | Start Date | Duration | Responsible | Sample Size / Target |
|---|-----------------|--------|-----------|-------------------|-------------|-----------|------------|----------|-------------|--------------------|
| 1 | Dashboard development cycle time | Days from requirements kickoff to production deployment | Continuous | Automated JIRA timestamps extraction; Manual verification | JIRA project management system; Project logs | Weekly | Nov 1, 2025 | 12 weeks | Raj Patel | 30 completed projects (6-month history) |
| 2 | Requirements rework incidents | Number of UAT change requests requiring code/design changes | Discrete (count) | UAT feedback form; Change request tracking | Azure DevOps change request log; UAT session notes | Per project completion | Nov 1, 2025 | Ongoing | James Murphy | All projects initiated (target: 15-20 projects) |
| 3 | Requirements rework rate | Percentage of projects requiring significant revision during UAT | Discrete (%) | Calculated as: (Projects with rework / Total projects) Ã— 100 | JIRA + Azure DevOps data | Weekly | Nov 1, 2025 | 12 weeks | James Murphy | Minimum 20 projects for statistical validity |
| 4 | Stakeholder satisfaction score | Net Promoter Score (NPS) measuring stakeholder satisfaction with delivery process | Ordinal (0-10 scale) | Post-deployment survey via Microsoft Forms (5-question) | Survey responses from stakeholder | Per project completion | Nov 1, 2025 | Ongoing | Emma O'Brien | Target: 45+ responses (all stakeholders) |
| 5 | On-time delivery rate | Percentage of dashboards delivered by committed deadline | Discrete (Yes/No) | JIRA milestone tracking; Actual vs. planned completion date comparison | JIRA project dates; Project tracker | Weekly | Nov 1, 2025 | 12 weeks | Raj Patel | All projects (minimum 20 projects) |
| 6 | Requirements approval time | Days from requirements documentation completion to stakeholder sign-off | Continuous | SharePoint approval workflow timestamps; Manual log | SharePoint approval logs; Project records | Per project | Nov 1, 2025 | Ongoing | Raj Patel | All projects in measurement period |
| 7 | Stakeholder meeting count | Number of requirements gathering meetings needed | Discrete (count) | Calendar invites; Meeting notes from attendees | Outlook calendar; Project meeting tracker | Per project | Nov 1, 2025 | Ongoing | Sai Sajjanam | All projects in measurement period |
| 8 | Dashboard complexity score | Subjective assessment of dashboard complexity (1=Very simple, 5=Very complex) | Ordinal (1-5) | Team assessment form completed at project kickoff | Project kickoff meeting discussion; Complexity rubric | Per project kickoff | Nov 1, 2025 | Ongoing | Emma O'Brien | All projects in measurement period |
| 9 | Data quality issues | Number of data accuracy or refresh issues discovered post-deployment | Discrete (count) | User feedback form; Production support tickets | ServiceNow support ticket system; User feedback | Weekly | Nov 1, 2025 | 12 weeks | Maria Santos | All issues reported |
| 10 | Team resource allocation | Actual hours spent by each team member per project | Continuous | Timesheet entries; JIRA time-logging | JIRA time tracking system; HR timesheet data | Per project completion | Nov 1, 2025 | Ongoing | Sai Sajjanam | All projects in measurement period |

---

## Data Type Definitions

| Data Type | Definition | Examples from This Project |
|-----------|-----------|---------------------------|
| **Continuous** | Can take any value within a range; infinite precision | Dashboard cycle time (32.5 days), Approval time (4.2 days) |
| **Discrete** | Countable whole numbers only | Rework incidents (3 changes), Meeting count (2 meetings) |
| **Ordinal** | Ranked categories with logical order | NPS score (1-10), Complexity score (1-5) |
| **Nominal** | Categories without inherent order | Department name, Dashboard type |
| **Percent (%)** | Ratio expressed as percentage | Rework rate (15%), On-time delivery (65%) |

---

## Data Quality Assurance

### Accuracy
- JIRA timestamps verified against project logs weekly
- Manual spot-checks of 10% of extracted data
- Conflicts between data sources investigated and resolved within 48 hours

### Completeness
- Missing data flagged daily and investigated
- Reason for missing data documented (e.g., project cancelled, data unavailable)
- 100% completion rate target for critical metrics

### Consistency
- Standardized definitions documented and shared with all collectors
- Weekly calibration meetings with data collectors to ensure consistent interpretation
- Data dictionary maintained and updated monthly

### Timeliness
- Data entered within 24 hours of collection event
- Weekly data compilation deadline: Friday 4:00 PM GMT
- Monthly summary reports prepared by 1st business day of following month

### Validity
- Logic checks: Cycle time cannot be negative; On-time rate must be 0-100%
- Outliers investigated (e.g., cycle time > 60 days)
- Comparison to historical ranges identifies anomalies

---

## Baseline Data Collection Schedule

| Week | Measurement Activity | Data to Collect | Responsible |
|------|----------------------|-----------------|-------------|
| 1-2 (Nov 1-15) | Historical data extraction | Past 30 projects: cycle time, rework rate, satisfaction | Raj Patel, James Murphy |
| 2-3 (Nov 8-22) | Current process validation | Ongoing projects: confirm collection methods | Sai Sajjanam |
| 3-8 (Nov 15 - Dec 27) | Baseline data collection | 12+ new projects: all 10 metrics | All team members |
| 9-12 (Jan 1-Feb 28) | Analysis & validation | Verify data quality; Calculate statistics | Sai Sajjanam |

---

## Data Storage & Confidentiality

- All data stored in secure SharePoint folder (access: Team members only)
- Raw data backed up weekly to OneDrive
- Personally identifiable information (stakeholder names) separated from metrics data
- Data access log maintained for audit purposes
- Retention: Data kept for project duration + 1 year per company policy

---

# 4. PROCESS MAP

## SIPOC Diagram (Supplier-Input-Process-Output-Customer)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SUPPLIERS    â”‚ INPUTS              â”‚ PROCESS           â”‚ OUTPUTS            â”‚ CUSTOMERS     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              â”‚                     â”‚                   â”‚                    â”‚               â”‚
â”‚ Business     â”‚ Dashboard request   â”‚ POWER BI          â”‚ Deployed           â”‚ Business      â”‚
â”‚ Stakeholders â”‚ Business need       â”‚ DASHBOARD         â”‚ dashboard          â”‚ Stakeholders  â”‚
â”‚              â”‚ Success metrics     â”‚ DEVELOPMENT       â”‚ User training      â”‚ (Finance,     â”‚
â”‚ Data         â”‚ Data sources list   â”‚ PROCESS           â”‚ Documentation      â”‚ Sales, Ops)   â”‚
â”‚ Engineers    â”‚ ETL specifications  â”‚                   â”‚                    â”‚               â”‚
â”‚              â”‚ Business rules      â”‚ DMAIC phases:     â”‚ Access              â”‚ IT Support    â”‚
â”‚ IT           â”‚ Power BI licenses   â”‚ 1) Requirements   â”‚ permissions        â”‚ Team          â”‚
â”‚ Infrastructureâ”‚ Server capacity     â”‚ 2) Data sourcing  â”‚ Lessons learned    â”‚               â”‚
â”‚              â”‚ Access permissions  â”‚ 3) Design & Dev   â”‚ Training materials â”‚ Future        â”‚
â”‚ Previous     â”‚ Best practices      â”‚ 4) Testing        â”‚                    â”‚ project       â”‚
â”‚ projects     â”‚ Templates           â”‚ 5) Deployment     â”‚                    â”‚ teams         â”‚
â”‚              â”‚ Lessons learned     â”‚ 6) Handover       â”‚                    â”‚               â”‚
â”‚              â”‚                     â”‚                   â”‚                    â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Detailed Process Flowchart

```
                         START: DASHBOARD REQUEST
                                  |
                                  v
                    1. REQUEST SUBMISSION
                    (Stakeholder portal form)
                                  |
                                  v
                    2. INITIAL ASSESSMENT
                    (Team Lead reviews request)
                                  |
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        v                   v
                    APPROVE        REJECT/CLARIFY
                        |                   |
                        v                   v
                   (Proceed)      (Return to stakeholder)
                        |
                        v
         â±ï¸  3. KICKOFF MEETING SCHEDULING
            (Average wait: 5 days)
                        |
                        v
         ğŸ• 4. KICKOFF MEETING CONDUCTED
            (Duration: 2 hours)
                        |
                        v
  â±ï¸  5. REQUIREMENTS GATHERING (CURRENT: 12 DAYS)
      ğŸ“Š Stakeholder meetings (2-3 sessions)
         - Discuss data needs
         - Identify metrics & KPIs
         - Understand visualizations required
                        |
                        v
      6. REQUIREMENTS DOCUMENTATION
         BA creates detailed requirements doc
                        |
                        v
  â±ï¸  7. STAKEHOLDER REVIEW & APPROVAL (CURRENT: 5 DAYS)
         - Stakeholder reviews doc
         - Provides feedback/approval
                        |
                        v
      8. DATA SOURCE IDENTIFICATION
         (Duration: 3 days)
         - Data Engineer evaluates sources
         - Checks data availability
                        |
                        v
  â±ï¸  9. ETL DEVELOPMENT (Duration: 8 days)
         - Build data pipelines in Informatica
         - Extract, Transform, Load to DW
                        |
                        v
  â±ï¸  10. DASHBOARD DESIGN & DEVELOPMENT (Duration: 10 days)
          - Power BI Designer creates layout
          - Developer builds visualizations
          - DAX calculations developed
                        |
                        v
      11. INTERNAL QA TESTING
          (Duration: 3 days)
          - Test functionality
          - Validate data accuracy
          - Performance check
                        |
                        v
  â±ï¸  12. USER ACCEPTANCE TESTING - UAT (CURRENT: 5 DAYS)
         - Stakeholder tests in staging
         - Provides feedback
                        |
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                v                v
            PASS            FAIL (Issues found)
                |                |
                v                v
           (Proceed)    â±ï¸  13. REWORK
                        (Average: 7 days when required)
                        (Current frequency: 15% of projects)
                        |
                        v
                    (Re-test UAT)
                        |
                        v
      14. DEPLOYMENT TO PRODUCTION
          (Duration: 1 day)
          - Deploy to Prod Power BI
          - Configure access permissions
                        |
                        v
      15. USER TRAINING & HANDOVER
          (Duration: 1 day)
          - Train end users
          - Share documentation
                        |
                        v
      16. PROJECT CLOSURE
          (Duration: 1 day)
          - Document lessons learned
          - Archive project files
          - Close project formally
                        |
                        v
                    END: DASHBOARD LIVE
                  Total Cycle: 45 days (avg)
                            Range: 35-60 days
```

---

## Process Metrics & Bottlenecks

| Phase | Current Duration | Benchmark | Gap | Status | Root Causes |
|-------|-----------------|-----------|-----|--------|------------|
| 1-2: Initial Assessment | 5 days | 2 days | +3 days | âš ï¸ | Limited intake capacity; Ad-hoc scheduling |
| 3-5: Requirements Gathering | 12 days | 5 days | +7 days | ğŸ”´ | Multiple meetings needed; Inconsistent documentation; Limited stakeholder availability |
| 6-7: Documentation & Approval | 5 days | 2 days | +3 days | âš ï¸ | No standardized templates; Long approval loops |
| 8-10: Data & Development | 21 days | 18 days | +3 days | âš ï¸ | Waiting for data sources; Rework due to unclear specs |
| 11-12: Testing & UAT | 8 days | 5 days | +3 days | âš ï¸ | 15% rework rate adding 7 days average |
| 13-16: Deployment & Closure | 3 days | 2 days | +1 day | âœ“ | Process efficient |
| **Total** | **45 days** | **32 days** | **+13 days** | ğŸ”´ | Multiple coordination points; Rework cycle |

---

## Key Process Observations

### High-Variation Areas (Root Causes of Extended Cycle Time)
1. **Requirements gathering (12 days vs. 5-day target):**
   - No standardized requirements template
   - Multiple iterations of requirements meetings
   - Unclear definition of "requirements complete"
   - Stakeholder unavailability causes delays

2. **Rework during UAT (15% of projects; +7 days when required):**
   - Incomplete or ambiguous initial requirements
   - Stakeholders change minds during testing
   - Inadequate UAT planning/preparation
   - UAT environment differs from production

3. **Approval delays (5 days vs. 2-day target):**
   - No formal approval workflow
   - Stakeholders reviewing async (no deadline)
   - Multiple approval levels not clearly defined

### Process Strengths
- Clear handoff between phases (low rework in design/development)
- Good internal QA catches issues before UAT
- Professional deployment and training process

---

# 5. HYPOTHESES

## Hypothesis 1: Dashboard Development Cycle Time

**Metric:** Dashboard development cycle time (days from requirements kickoff to production deployment)

**Baseline:** 45 days (current average) | **Target:** 32 days (29% reduction)

### Null Hypothesis (Hâ‚€):
Implementing standardized requirements templates, structured stakeholder engagement, and Agile sprint methodology will have **no statistically significant effect** on the average dashboard development cycle time. The mean cycle time will remain at 45 days (Î¼ = 45).

### Alternative Hypothesis (Hâ‚):
Implementing standardized requirements templates, structured stakeholder engagement, and Agile sprint methodology will **significantly reduce** the average dashboard development cycle time. The mean cycle time will decrease to 32 days or less (Î¼ â‰¤ 32).

### Statistical Test:
**Two-sample t-test** (Baseline period vs. Post-improvement period)
- Sample size: Minimum 20 projects per group (recommend 25-30 projects)
- Significance level: Î± = 0.05 (95% confidence)
- Test type: One-tailed test (directional: expecting reduction)

### Success Criteria:
- P-value < 0.05 (statistically significant improvement)
- Mean cycle time reduction of at least 29% (13+ days)
- Standard deviation reduced (indicates more consistent delivery)

---

## Hypothesis 2: Requirements Rework Rate

**Metric:** Percentage of projects requiring significant revision during UAT

**Baseline:** 15% rework rate (current) | **Target:** 6% rework rate (60% reduction)

### Null Hypothesis (Hâ‚€):
Using structured requirements templates with formal stakeholder sign-off will have **no significant effect** on the rework rate. The proportion of projects requiring rework will remain at 15% (p = 0.15).

### Alternative Hypothesis (Hâ‚):
Using structured requirements templates with formal stakeholder sign-off will **significantly reduce** the rework rate to 6% or lower (p â‰¤ 0.06).

### Statistical Test:
**Two-proportion z-test** (Chi-square test alternative)
- Sample size: Minimum 20 projects per group (both baseline and post-implementation)
- Significance level: Î± = 0.05 (95% confidence)
- Test type: One-tailed test (directional: expecting reduction)

### Success Criteria:
- P-value < 0.05 (statistically significant improvement)
- Rework rate reduction from 15% to 6% or lower
- Reduced average rework duration (from 7 days to < 3 days when rework occurs)

---

## Hypothesis 3: Stakeholder Satisfaction Score (NPS)

**Metric:** Net Promoter Score (NPS) measuring stakeholder satisfaction

**Baseline:** 6.2/10 NPS (current) | **Target:** 8.5/10 NPS (37% improvement)

### Null Hypothesis (Hâ‚€):
Process improvements (standardization, faster delivery, clearer communication) will have **no significant effect** on stakeholder satisfaction. The mean NPS will remain at 6.2/10 (Î¼ = 6.2).

### Alternative Hypothesis (Hâ‚):
Process improvements will **significantly increase** stakeholder satisfaction. The mean NPS will increase to 8.5/10 or higher (Î¼ â‰¥ 8.5).

### Statistical Test:
**Paired t-test** (Same stakeholders surveyed before and after; captures individual improvement)
- Sample size: Minimum 25 stakeholders surveyed in both periods
- Significance level: Î± = 0.05 (95% confidence)
- Test type: One-tailed test (directional: expecting improvement)

### Success Criteria:
- P-value < 0.05 (statistically significant improvement)
- Mean NPS increase of at least 2.3 points (to â‰¥ 8.5)
- % Promoters increase from 20% to â‰¥ 50%
- % Detractors decrease from 35% to â‰¤ 15%

---

## Hypothesis 4: On-Time Delivery Rate

**Metric:** Percentage of dashboards delivered by committed deadline

**Baseline:** 65% on-time delivery (current) | **Target:** 90% on-time delivery (38% improvement)

### Null Hypothesis (Hâ‚€):
Implementing realistic project timelines and Agile sprint planning will have **no significant effect** on on-time delivery rate. The proportion on-time will remain at 65% (p = 0.65).

### Alternative Hypothesis (Hâ‚):
Implementing realistic project timelines and Agile sprint planning will **significantly improve** on-time delivery rate to 90% or higher (p â‰¥ 0.90).

### Statistical Test:
**Two-proportion z-test**
- Sample size: Minimum 20 projects per group
- Significance level: Î± = 0.05 (95% confidence)
- Test type: One-tailed test (directional: expecting improvement)

### Success Criteria:
- P-value < 0.05 (statistically significant improvement)
- On-time delivery rate increase from 65% to â‰¥ 90%
- Fewer schedule overruns (only 1 out of 10 projects delayed)
- Average delay time reduced when overruns occur

---

## Hypothesis 5: Requirements Approval Time

**Metric:** Days from requirements documentation completion to stakeholder sign-off

**Baseline:** 12 days approval time (current) | **Target:** 5 days approval time (58% reduction)

### Null Hypothesis (Hâ‚€):
Streamlining the approval process with automated SharePoint workflows and clear approval criteria will have **no significant effect** on approval time. The mean approval time will remain at 12 days (Î¼ = 12).

### Alternative Hypothesis (Hâ‚):
Streamlining the approval process with automated SharePoint workflows and clear approval criteria will **significantly reduce** approval time. The mean approval time will decrease to 5 days or less (Î¼ â‰¤ 5).

### Statistical Test:
**Two-sample t-test** (Baseline vs. Post-improvement)
- Sample size: Minimum 20 approvals per group
- Significance level: Î± = 0.05 (95% confidence)
- Test type: One-tailed test (directional: expecting reduction)

### Success Criteria:
- P-value < 0.05 (statistically significant improvement)
- Mean approval time reduction of 7+ days (to â‰¤ 5 days)
- Standard deviation reduced (more consistent approval timelines)
- 95% of approvals completed within 7 days (vs. current 65%)

---

## Summary: Hypothesis Testing Framework

| Hypothesis | Baseline | Target | Test Type | Sample | Î± | Expected Outcome |
|-----------|----------|--------|-----------|--------|---|-----------------|
| H1: Cycle time | 45 days | 32 days | Two-sample t-test | 20-30 projects/group | 0.05 | Î¼â‚ < Î¼â‚‚ |
| H2: Rework rate | 15% | 6% | Two-proportion z-test | 20+ projects/group | 0.05 | pâ‚ < pâ‚‚ |
| H3: NPS satisfaction | 6.2/10 | 8.5/10 | Paired t-test | 25+ same stakeholders | 0.05 | Î¼_after > Î¼_before |
| H4: On-time delivery | 65% | 90% | Two-proportion z-test | 20+ projects/group | 0.05 | pâ‚ < pâ‚‚ |
| H5: Approval time | 12 days | 5 days | Two-sample t-test | 20+ approvals/group | 0.05 | Î¼â‚ < Î¼â‚‚ |

---

# 6. REFLECTION: LESSONS LEARNED

## Lessons Learned from Six Sigma Yellow Belt Training and Capstone Project

The Six Sigma Yellow Belt training has fundamentally transformed my approach to process improvement, business problem-solving, and data-driven decision-making. Prior to this certification, my analytical work focused primarily on delivering technical solutionsâ€”building data dashboards, automating reports, optimizing queries, and extracting insights from large datasets. While these technical skills remain essential, the structured DMAIC (Define-Measure-Analyze-Improve-Control) methodology has taught me to step back, think systematically, and diagnose root causes before jumping to solutions.

### Key Insights from Training Phase

**The Discipline of Problem Definition:**
The most impactful lesson from the Define phase has been the importance of articulating problems with precision. In my previous roles at Goldman Sachs and other clients, I often addressed symptoms rather than root causes. The training emphasizing clear, measurable problem statements has changed my mindset. A well-defined problem statement should include: a time reference, the measurable gap (current vs. baseline), and quantified business impact. This discipline ensures improvement efforts target the right issues and that success can be objectively measured.

For this capstone project, instead of vaguely stating "dashboards take too long to build," I learned to define the problem as: "Dashboard development cycle time is 45 days, which is 30% longer than the industry benchmark of 32 days, resulting in â‚¬50K annual cost of lost productivity." This specificity immediately focuses team attention and enables measurement of progress.

**Data as the Foundation:**
The Measure phase has taught me that assumptions and intuition, while valuable, must be validated with baseline data. Previously, I might have jumped to solutions based on stakeholder complaints ("Requirements gathering takes forever"). The Six Sigma approach forced me to actually collect data on how long requirements gathering takes (12 days), compare it to industry benchmarks (5 days), calculate the impact (7-day gap Ã— number of projects Ã— labor cost), and define it clearly before proposing improvements.

This discipline has transformed how I approach analytics consulting. I now insist on establishing baseline metrics before recommending process changes. The data collection plan I developed for this projectâ€”specifying what to measure, collection methods, data types, frequency, and responsible partiesâ€”is a framework I will apply to all future projects.

**Hypothesis Testing as a Bridge Between Data and Action:**
Writing formal null and alternative hypotheses for each improvement measure has deepened my understanding of statistical thinking. Instead of assuming "if we implement standardized templates, cycle time will decrease," the Yellow Belt training taught me to frame this as a testable hypothesis with specific success criteria (e.g., 29% reduction with p-value < 0.05).

This approach is particularly valuable in consulting environments where multiple stakeholders may advocate for different solutions. A hypothesis-driven framework ensures decisions are based on validated evidence, not politics or preferences.

### Insights from the Capstone Project

**Process Mapping Reveals Invisible Inefficiencies:**
Creating the SIPOC diagram and detailed process flowchart for the Power BI dashboard development process was eye-opening. Before this exercise, I viewed dashboard development as a largely linear process. The flowchart revealed multiple coordination points, approval loops, and rework cycles that create variation and delay.

Specifically, I identified that 15% of projects experience rework during UAT, adding an average of 7 days to the schedule. Without the process map, this hidden inefficiency might have gone unnoticed. The SIPOC diagram also clarified the multiple supplier inputs (data engineers, IT infrastructure, stakeholders) and identified dependencies that could cause delays.

**Cross-Functional Teamwork is Critical:**
Developing this project plan with diverse team membersâ€”Power BI developer, business analyst, data engineer, and QA analystâ€”has demonstrated the value of diverse perspectives. Each team member brought insights about their phase of the process that I, as a technology-focused analyst, might have missed.

For example, the Business Analyst highlighted that requirements gatherings often require 2-3 meetings because stakeholders haven't clearly thought through their needs upfront. The QA Analyst noted that 80% of UAT issues stem from ambiguous requirements rather than development bugs. These cross-functional insights shaped the hypotheses and data collection plan in ways that a solo analyst approach would have missed.

**Real-World Application vs. Academic Exercise:**
What initially felt like an academic exerciseâ€”creating charters, writing hypotheses, building data collection plansâ€”has proven to be directly applicable to real-world consulting and business analyst roles. The Project Charter template is exactly the type of document that consulting firms like PwC, McKinsey, or Deloitte would create at project kickoff. The Team Charter reflects actual team dynamics I've experienced in previous roles.

This capstone forced me to bridge theory and practice, ensuring I can credibly discuss Six Sigma concepts not just as certification knowledge, but as approaches I've actually applied to a realistic business scenario.

### Skills Developed That Will Serve My Career

**1. Business Problem Framing:**
I've learned to frame business problems in ways that unite teams around common goals. Instead of "improve dashboard development," the refined problem statement now emphasizes the financial impact (â‚¬50K annual cost) and stakeholder pain point (delayed strategic decisions). This framing is essential in consulting environments where you must secure buy-in from multiple stakeholders.

**2. Measurement and Baseline Thinking:**
Six Sigma's emphasis on measurement has reinforced a discipline I've developed in data analytics workâ€”establish baselines before claiming success. This prevents the "we think we're better" trap that organizations often fall into without data validation. I will carry this baseline-thinking approach to all future analytical projects.

**3. Cross-Functional Leadership:**
Developing this project plan with five team members has demonstrated that effective process improvement isn't a solo analyst activityâ€”it requires coordination, clear communication, and alignment across diverse functions. The Team Charter I developed outlines specific norms (data-driven decisions, respectful disagreement, clear communication) that foster team effectiveness. I've learned these are not nice-to-haves but essential foundations for project success.

**4. Hypothesis-Driven Thinking:**
The shift from "I think we should do X" to "Here's the hypothesis, here's how we'll test it, and here's what success looks like" is a transformational mindset change. This discipline will make me a more credible advisor in consulting roles, where decision-makers need confidence that recommendations are evidence-based.

### How This Prepares Me for Consulting Roles (PwC, McKinsey, Deloitte)

This Yellow Belt training and capstone project have prepared me well for advisory and consulting roles where process improvement is a core service offering:

- **Client Engagement:** I can now engage clients not just on technical analytics capabilities but on how to structure process improvement initiatives using a proven framework (DMAIC).

- **Scope Definition:** The Project Charter provides a template for defining client engagementsâ€”clearly articulating the problem, business case, goals, scope, and timeline. This discipline prevents scope creep and misalignment.

- **Team Dynamics:** My experience building and managing this cross-functional team provides a foundation for consulting roles where I'll lead diverse teams across multiple client engagements.

- **Measurable Value:** By emphasizing baseline metrics and hypothesis testing, I can help clients move beyond "We think we're better" to "Here's the data proving we improved." This shifts consulting from subjective advice to validated outcomesâ€”a critical differentiator for top consulting firms.

- **Continuous Learning:** The fact that I pursued Yellow Belt certification while building my analytics career demonstrates commitment to continuous skill development. This mindset aligns with the learning culture of leading consulting firms.

### Areas for Further Growth

While Yellow Belt training has equipped me to scope and measure process improvements, it has explicitly excluded the Improve and Control phases. To progress toward Black Belt capability, I would benefit from:

1. **Hands-on experience** implementing improvements (not just recommending them)
2. **Statistical software proficiency** (Minitab, JMP, or R for Six Sigma analysis)
3. **Advanced statistical techniques** (design of experiments, regression analysis, advanced process capability)
4. **Change management** experienceâ€”process improvements fail without effective stakeholder adoption

These are areas I'll pursue in future Green Belt and Black Belt projects or roles.

### Conclusion

The Six Sigma Yellow Belt training has not simply added another credential to my resumeâ€”it has fundamentally reshaped how I think about business problems, data analysis, and improvement initiatives. The structured DMAIC framework, emphasis on measurement, and discipline of hypothesis testing have made me a more effective analytical thinker and more credible advisor.

This capstone project, focusing on BI dashboard development process improvement, demonstrates that these principles apply directly to my domain expertise (business intelligence and analytics) and the environments where I aspire to work (consulting and advisory firms). I'm excited to apply these methodologies to real-world client challenges and contribute to organizations' operational excellence through data-driven, systematically-designed process improvements.

---

## END OF CAPSTONE DOCUMENT

**Document Version:** 1.0  
**Date:** October 28, 2025  
**Prepared by:** Sai Krishna Sajjanam  
**Project:** Power BI Dashboard Development Process Optimization  
**Status:** Ready for Peer Review
"""